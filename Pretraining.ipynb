{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretraining IotBert "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from tokenizers.implementations import ByteLevelBPETokenizer\n",
    "from tokenizers.processors import BertProcessing\n",
    "from pathlib import Path\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that we have a GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaConfig\n",
    "from transformers import RobertaForMaskedLM\n",
    "config = RobertaConfig(\n",
    "    vocab_size=52_000,\n",
    "    max_position_embeddings=578,\n",
    "    num_attention_heads=12,\n",
    "    num_hidden_layers=6,\n",
    "    type_vocab_size=1,\n",
    ")\n",
    "\n",
    "model = RobertaForMaskedLM(config=config)\n",
    "model.num_parameters() # => 83.5 million parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizerFast\n",
    "\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained(\"./models\", max_len=576)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load/Save pre-training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class IoTDataset(Dataset):\n",
    "    def __init__(self, evaluate: bool = False):\n",
    "        tokenizer = ByteLevelBPETokenizer(\n",
    "            \"./models/vocab.json\",\n",
    "            \"./models/merges.txt\",\n",
    "        )\n",
    "        tokenizer._tokenizer.post_processor = BertProcessing(\n",
    "            (\"</s>\", tokenizer.token_to_id(\"</s>\")),\n",
    "            (\"<s>\", tokenizer.token_to_id(\"<s>\")),\n",
    "        )\n",
    "        tokenizer.enable_truncation(max_length=576)\n",
    "        # or use the RobertaTokenizer from `transformers` directly.\n",
    "\n",
    "        self.examples = []\n",
    "\n",
    "        src_files = Path(\"./data/\").glob(\"*_7.csv\") if evaluate else Path(\"./data/\").glob(\"*.csv\")\n",
    "        for src_file in src_files:\n",
    "            print(\"ðŸ”¥\", src_file)\n",
    "            lines = src_file.read_text(encoding=\"utf-8\").splitlines()\n",
    "            self.examples += [x.ids for x in tokenizer.encode_batch(lines)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.examples)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # Weâ€™ll pad at the batch level.\n",
    "        return torch.tensor(self.examples[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=IoTDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dumping pretraning dataset into pickle\n",
    "\n",
    "import pickle\n",
    "pickle.dump( data, open( \"./data/tokenizer/dataset-drapgh.pkl\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading pretraining dataset from pickle\n",
    "import pickle\n",
    "\n",
    "data = pickle.load(open( \"./data/tokenizer/dataset-drapgh.pkl\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    do_train=True,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=8, # increase batch_size 32 x 8 =256  nice doh  gradient accumulation\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=5,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "print(now)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save final model (+ tokenizer + config) to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model(\"./models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the LM actually trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "fill_mask = pipeline(\n",
    "    \"fill-mask\",\n",
    "    model=\"./models\",\n",
    "    tokenizer=\"./models\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"sp:4856 dp:49152 <mask>.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fill_mask(\"sp:4856 <mask> ptcl:6 ipv:4 vln:0 tnnl:0 bi_dur:34 bi_pkt:14 bi_byte:4544 s2d_dur:32 s2d:7 s2d_byte:555 d2s_dur:33 d2s:7 d2s_byte:3989 bi_min_ps:52 bi_mean_ps:324.57 bi_std_ps:516.42 bi_max_ps:1500 s2d_min_ps:52 s2d_mean_ps:79.29 s2d_std_ps:68.73 s2d_max_ps:235 d2s_min_ps:52 d2s_mean_ps:569.86 d2s_std_ps:657.82 d2s_max_ps:1500 bi_min_pi_ms:0 bi_mean_pi_ms:2.62 bi_std_pi_ms:2.26 bi_max_pi_ms:6 s2d_min_pi_ms:1 s2d_mean_pi_ms:5.33 s2d_std_pi_ms:5.5 s2d_max_pi_ms:16 d2s_min_pi_ms:0 d2s_mean_pi_ms:5.5 d2s_std_pi_ms:6.06 d2s_max_pi_ms:16 bi_syn:2 bi_cwr:0 bi_ece:0 bi_urg:0 bi_ack:13 bi_psh:3 bi_rst:0 bi_fin:2 s2d_syn:1 s2d_cwr:0 s2d_ece:0 s2d_urg:0 s2d_ack:6 s2d_psh:1 s2d_rst:0 s2d_fin:1 d2s_syn:1 d2s_cwr:0 d2s_ece:0 d2s_urg:0 d2s_ack:7 d2s_psh:2 d2s_rst:0 d2s_fin:1 app_name:HTTP app_cat:Web req_server_name:192.168.1.223 client_fingerprint:nan server_fingerprint:nan content_type:text/xml\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('transformer-env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6250c60fce2b1b45352618ca667374ea37a97a266b04c0c4db6f8ef62c92bb72"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
